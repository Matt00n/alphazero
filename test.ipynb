{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "[1.]\n",
      "(1,)\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "terminated = jnp.logical_or(\n",
    "    1 < 2,\n",
    "    3 < 4,\n",
    ")\n",
    "terminated = jnp.where(\n",
    "    terminated, jnp.ones(1), jnp.zeros(1)\n",
    ").astype(float)\n",
    "truncated = jnp.where(\n",
    "    5 >= 4, 1 - terminated, jnp.zeros_like(terminated)\n",
    ")\n",
    "\n",
    "print(terminated.shape)\n",
    "print(terminated)\n",
    "print(truncated.shape)\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([4.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.ones(1) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*128*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/module.py:77: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  KeyArray = Union[jax.Array, jax.random.KeyArray]  # pylint: disable=invalid-name\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/recurrent.py:45: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  PRNGKey = jax.random.KeyArray\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/recurrent.py:753: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  init_key: Optional[random.KeyArray] = None,\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/recurrent.py:979: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  init_key: Optional[random.KeyArray] = None,\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/recurrent.py:1003: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  init_key: Optional[random.KeyArray] = None,\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/flax/linen/stochastic.py:28: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
      "  KeyArray = Union[jax.Array, jax.random.KeyArray]\n",
      "/Users/matthiaslehmann/miniforge3/envs/rl_algo/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
      "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from envs import make_env, Transition, MCTSTransition, has_discrete_action_space, is_atari_env\n",
    "# from envs.brax_v1_wrappers import wrap_for_training\n",
    "from envs.brax_wrappers import EvalWrapper, wrap_for_training\n",
    "from networks.policy import Policy, ForwardPass\n",
    "from networks.networks import FeedForwardNetwork, ActivationFn, make_policy_network, make_value_network, make_atari_feature_extractor\n",
    "from networks.distributions import NormalTanhDistribution, ParametricDistribution, PolicyNormalDistribution, DiscreteDistribution\n",
    "import replay_buffers\n",
    "import running_statistics\n",
    "from gymnax import gymnax\n",
    "from gymnax.gymnax.wrappers.brax import GymnaxToBraxWrapper, State\n",
    "import mctx\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "is_atari = is_atari_env('CartPole-v1')\n",
    "environment, env_params = gymnax.make('CartPole-v1')\n",
    "discrete_action_space = has_discrete_action_space(environment, env_params)\n",
    "if not discrete_action_space:\n",
    "    raise NotImplementedError('Currently only discrete action spaces are supported.')\n",
    "environment = GymnaxToBraxWrapper(environment)\n",
    "\n",
    "env = wrap_for_training(\n",
    "    environment,\n",
    "    episode_length=500,\n",
    "    action_repeat=1,\n",
    ")\n",
    "key = jax.random.PRNGKey(42)\n",
    "key_envs, key = jax.random.split(key, 2)\n",
    "reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "key_envs = jax.random.split(key_envs, 8 // 1)\n",
    "key_envs = jnp.reshape(key_envs,\n",
    "                        (1, -1) + key_envs.shape[1:])\n",
    "env_state = reset_fn(key_envs)\n",
    "\n",
    "action_size = env.action_size()\n",
    "\n",
    "if is_atari:\n",
    "    observation_shape = env_state.obs.shape[-3:]\n",
    "else:\n",
    "    observation_shape = env_state.obs.shape[-1:]\n",
    "\n",
    "dummy_obs = jnp.zeros(observation_shape,)\n",
    "dummy_action = jnp.zeros((action_size,))\n",
    "dummy_transition = MCTSTransition(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "    observation=dummy_obs,\n",
    "    action=dummy_action,\n",
    "    reward=0.,\n",
    "    discount=0.,\n",
    "    next_observation=dummy_obs,\n",
    "    target_policy_probs=jnp.zeros((action_size,)),\n",
    "    target_value=0.,\n",
    "    extras={\n",
    "        'state_extras': {\n",
    "            'truncation': 0.\n",
    "        },\n",
    "        'policy_extras': {\n",
    "            'prior_log_prob': dummy_action,\n",
    "            'raw_action': dummy_action\n",
    "        }\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTSTransition(observation=Array([0., 0., 0., 0.], dtype=float32), action=Array([0., 0.], dtype=float32), reward=0.0, discount=0.0, next_observation=Array([0., 0., 0., 0.], dtype=float32), target_policy_probs=Array([0., 0.], dtype=float32), target_value=0.0, extras={'state_extras': {'truncation': 0.0}, 'policy_extras': {'prior_log_prob': Array([0., 0.], dtype=float32), 'raw_action': Array([0., 0.], dtype=float32)}})\n",
      "(20,)\n",
      "MCTSTransition(observation=Array([0., 0., 0., 0.], dtype=float32), action=Array([0., 0.], dtype=float32), reward=Array(0., dtype=float32), discount=Array(0., dtype=float32), next_observation=Array([0., 0., 0., 0.], dtype=float32), target_policy_probs=Array([0., 0.], dtype=float32), target_value=Array(0., dtype=float32), extras={'policy_extras': {'prior_log_prob': Array([0., 0.], dtype=float32), 'raw_action': Array([0., 0.], dtype=float32)}, 'state_extras': {'truncation': Array(0., dtype=float32)}})\n"
     ]
    }
   ],
   "source": [
    "dummy_flatten, _unflatten_fn = jax.flatten_util.ravel_pytree(\n",
    "        dummy_transition\n",
    "    )\n",
    "\n",
    "print(dummy_transition)\n",
    "print(dummy_flatten.shape)\n",
    "print(_unflatten_fn(dummy_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, logits_rng, search_rng = jax.random.split(key, 3)\n",
    "\n",
    "# logits at root produced by the prior policy \n",
    "def forward()\n",
    "prior_logits, value = forward(env_state.obs)\n",
    "\n",
    "use_mixed_value = False\n",
    "\n",
    "# NOTE: For AlphaZero embedding is env_state, for MuZero\n",
    "# the root output would be the output of MuZero representation network.\n",
    "root = mctx.RootFnOutput(\n",
    "    prior_logits=prior_logits,\n",
    "    value=value,\n",
    "    # The embedding is used only to implement the MuZero model.\n",
    "    embedding=env_state, \n",
    ")\n",
    "\n",
    "# The recurrent_fn is provided by MuZero dynamics network.\n",
    "# Or true environment for AlphaZero\n",
    "# TODO MCTS: pass in dynamics function for MuZero\n",
    "def recurrent_fn(params, rng_key, action, embedding):\n",
    "    # environment (model)\n",
    "    env_state = embedding\n",
    "    nstate = env.step(env_state, action)\n",
    "\n",
    "    # policy & value networks\n",
    "    prior_logits, value = forward(env_state.obs)\n",
    "\n",
    "    # Create the new MCTS node.\n",
    "    recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "        reward=nstate.reward,\n",
    "        # discount when terminal state reached\n",
    "        discount=1 - nstate.done,\n",
    "        # prior for the new state\n",
    "        prior_logits=prior_logits,\n",
    "        # value for the new state\n",
    "        value=value,\n",
    "    )\n",
    "\n",
    "    # Return the new node and the new environment.\n",
    "    return recurrent_fn_output, nstate\n",
    "\n",
    "# Running the search.\n",
    "policy_output = mctx.gumbel_muzero_policy(\n",
    "    params=(),\n",
    "    rng_key=search_rng,\n",
    "    root=root,\n",
    "    recurrent_fn=recurrent_fn,\n",
    "    num_simulations=30,\n",
    "    max_num_considered_actions=16,\n",
    "    qtransform=partial(\n",
    "        mctx.qtransform_completed_by_mix_value,\n",
    "        use_mixed_value=use_mixed_value),\n",
    ")\n",
    "\n",
    "actions = policy_output.action\n",
    "action_weights = policy_output.action_weights\n",
    "best_actions = jnp.argmax(action_weights, axis=-1).astype(jnp.int32)\n",
    "actions = jax.lax.select(deterministic_actions, best_actions, actions)\n",
    "\n",
    "search_value = policy_output.search_tree.summary().value\n",
    "\n",
    "policy_extras = {\n",
    "    'prior_log_prob': tfd.Categorical(logits=prior_logits).log_prob(actions),\n",
    "    'raw_action': actions\n",
    "}\n",
    "\n",
    "nstate = env.step(env_state, actions)\n",
    "state_extras = {x: nstate.info[x] for x in extra_fields}\n",
    "return nstate, MCTSTransition(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "    observation=env_state.obs,\n",
    "    action=actions,\n",
    "    reward=nstate.reward,\n",
    "    discount=1 - nstate.done,\n",
    "    next_observation=nstate.obs,\n",
    "    target_policy_probs=action_weights,\n",
    "    target_value=search_value,\n",
    "    extras={\n",
    "        'policy_extras': policy_extras, \n",
    "        'state_extras': state_extras\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
