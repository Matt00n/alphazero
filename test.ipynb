{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "sample_dist_logits = jnp.array([[1,2,3,4], [3,3,-50,1], [1,1,1,1]], dtype=float)\n",
    "empirical_logits = jnp.zeros_like(sample_dist_logits)\n",
    "\n",
    "sample_dist_logits = jnp.expand_dims(sample_dist_logits, axis=1)\n",
    "batch_size = empirical_logits.shape[0]\n",
    "samples = jax.random.categorical(jax.random.PRNGKey(14), sample_dist_logits, shape=(batch_size, num_samples))\n",
    "\n",
    "# empirical_logits = empirical_logits.at[jnp.arange(batch_size), samples].add(1.)\n",
    "# empirical_logits = empirical_logits.at[samples].add(1.)\n",
    "# empirical_logits = empirical_logits.at[batch_size, samples].add(1.)\n",
    "\n",
    "def update(x, *indices):\n",
    "  return x.at[indices].add(1.)\n",
    "\n",
    "batch_update = jax.vmap(update)\n",
    "empirical_logits = batch_update(empirical_logits, samples) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0343 0.0847 0.2402 0.6408]\n",
      " [0.465  0.4692 0.     0.0658]\n",
      " [0.2522 0.2484 0.2537 0.2457]]\n",
      "[[[3.2058604e-02 8.7144323e-02 2.3688284e-01 6.4391428e-01]]\n",
      "\n",
      " [[4.6831053e-01 4.6831053e-01 4.4970365e-24 6.3378938e-02]]\n",
      "\n",
      " [[2.5000000e-01 2.5000000e-01 2.5000000e-01 2.5000000e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(empirical_logits)\n",
    "print(jax.nn.softmax(sample_dist_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9.3571980e-14 2.0610600e-09 4.5397868e-05 9.9995458e-01]]\n",
      "\n",
      " [[5.0000000e-01 5.0000000e-01 0.0000000e+00 1.0305768e-09]]\n",
      "\n",
      " [[2.5000000e-01 2.5000000e-01 2.5000000e-01 2.5000000e-01]]]\n",
      "[[[9.3571980e-14 2.0610600e-09 4.5397868e-05 9.9995458e-01]]\n",
      "\n",
      " [[5.0000000e-01 5.0000000e-01 0.0000000e+00 1.0305768e-09]]\n",
      "\n",
      " [[2.5000000e-01 2.5000000e-01 2.5000000e-01 2.5000000e-01]]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def _apply_temperature(logits, temperature):\n",
    "  \"\"\"Returns `logits / temperature`, supporting also temperature=0.\"\"\"\n",
    "  # The max subtraction prevents +inf after dividing by a small temperature.\n",
    "  logits = logits - jnp.max(logits, keepdims=True, axis=-1)\n",
    "  tiny = jnp.finfo(logits.dtype).tiny\n",
    "  return logits / jnp.maximum(tiny, temperature)\n",
    "\n",
    "temp = 0.1\n",
    "print(jax.nn.softmax(sample_dist_logits / temp))\n",
    "print(jax.nn.softmax(_apply_temperature(sample_dist_logits, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2],\n",
       "       [6]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "a = jnp.array([[[1],[2],[3]],[[4],[5],[6]]])\n",
    "a[jnp.arange(a.shape[0]), jnp.array([1,2]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  2.],\n",
       "       [  1.],\n",
       "       [ 38.],\n",
       "       [ 37.],\n",
       "       [ 36.],\n",
       "       [ 35.],\n",
       "       [  2.],\n",
       "       [  1.],\n",
       "       [102.],\n",
       "       [101.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def compute_gae(truncation: jnp.ndarray,\n",
    "                termination: jnp.ndarray,\n",
    "                rewards: jnp.ndarray,\n",
    "                values: jnp.ndarray,\n",
    "                bootstrap_value: jnp.ndarray,\n",
    "                lambda_: float = 1.0,\n",
    "                discount: float = 0.99):\n",
    "    \"\"\"Calculates the Generalized Advantage Estimation (GAE).\n",
    "\n",
    "    Args:\n",
    "        truncation: A float32 tensor of shape [T, B] with truncation signal.\n",
    "        termination: A float32 tensor of shape [T, B] with termination signal.\n",
    "        rewards: A float32 tensor of shape [T, B] containing rewards generated by\n",
    "        following the behaviour policy.\n",
    "        values: A float32 tensor of shape [T, B] with the value function estimates\n",
    "        wrt. the target policy.\n",
    "        bootstrap_value: A float32 of shape [B] with the value function estimate at\n",
    "        time T.\n",
    "        lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to\n",
    "        lambda_=1.\n",
    "        discount: TD discount.\n",
    "\n",
    "    Returns:\n",
    "        A float32 tensor of shape [T, B]. Can be used as target to\n",
    "        train a baseline (V(x_t) - vs_t)^2.\n",
    "        A float32 tensor of shape [T, B] of advantages.\n",
    "    \"\"\"\n",
    "\n",
    "    truncation_mask = 1 - truncation\n",
    "    # Append bootstrapped value to get [v1, ..., v_t+1]\n",
    "    values_t_plus_1 = jnp.concatenate(\n",
    "        [values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "    deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values\n",
    "    deltas *= truncation_mask\n",
    "\n",
    "    acc = jnp.zeros_like(bootstrap_value)\n",
    "    vs_minus_v_xs = []\n",
    "\n",
    "    def compute_vs_minus_v_xs(carry, target_t):\n",
    "        lambda_, acc = carry\n",
    "        truncation_mask, delta, termination = target_t\n",
    "        acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc\n",
    "        return (lambda_, acc), (acc)\n",
    "\n",
    "    (_, _), (vs_minus_v_xs) = jax.lax.scan(\n",
    "        compute_vs_minus_v_xs, (lambda_, acc),\n",
    "        (truncation_mask, deltas, termination),\n",
    "        length=int(truncation_mask.shape[0]),\n",
    "        reverse=True)\n",
    "    # Add V(x_s) to get v_s.\n",
    "    vs = jnp.add(vs_minus_v_xs, values)\n",
    "\n",
    "    vs_t_plus_1 = jnp.concatenate(\n",
    "        [vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "    advantages = (rewards + discount *\n",
    "                    (1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
    "    return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)\n",
    "\n",
    "truncation = jnp.array([[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],], dtype=float)\n",
    "termination = jnp.array([[0],[1],[0],[0],[0],[0],[0],[1],[0],[0],], dtype=float)\n",
    "rewards = jnp.array([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],], dtype=float)\n",
    "values = jnp.array([[10],[15],[20],[25],[30],[35],[40],[45],[50],[55],], dtype=float)\n",
    "bootstrap_value = jnp.array([100], dtype=float)\n",
    "lambda_ = 1.\n",
    "discount = 1.\n",
    "\n",
    "vs, adv = compute_gae(truncation,termination,rewards,values,bootstrap_value,lambda_,discount)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from envs.brax_wrappers import EvalWrapper, wrap_for_training, EpisodeWrapper, AutoResetWrapper\n",
    "from gymnax import gymnax\n",
    "from gymnax.gymnax.wrappers.brax import GymnaxToBraxWrapper\n",
    "\n",
    "\n",
    "local_key = jax.random.PRNGKey(42)\n",
    "local_key = jax.random.fold_in(local_key, 12)\n",
    "local_key, rb_key, key_envs, eval_key = jax.random.split(local_key, 4)\n",
    "environment, env_params = gymnax.make('CartPole-v1')\n",
    "environment = GymnaxToBraxWrapper(environment)\n",
    "\n",
    "env = wrap_for_training(\n",
    "    environment,\n",
    "    episode_length=10,\n",
    "    action_repeat=1,\n",
    ")\n",
    "env = EpisodeWrapper(environment, 10, 1)\n",
    "# env = VmapWrapper(env)\n",
    "env = AutoResetWrapper(env)\n",
    "\n",
    "reset_fn = jax.jit(env.reset)\n",
    "# key_envs = jax.random.split(key_envs, 4)\n",
    "# key_envs = jnp.reshape(key_envs,\n",
    "#                         (1, -1) + key_envs.shape[1:])\n",
    "env_state = reset_fn(key_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_obs\n"
     ]
    }
   ],
   "source": [
    "if 'steps' in env_state.info:\n",
    "    print('first_obs')\n",
    "env_state = env.step(env_state, jnp.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176.78586     3.4663894]\n",
      "[ 0 90]\n",
      "[176.78586  93.46639]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "c1 = 50\n",
    "c2 = 19652\n",
    "\n",
    "visit_counts = jnp.array([0,50])\n",
    "node_visit = jnp.sum(visit_counts)\n",
    "pb_c = c1 + jnp.log((node_visit + c2 + 1.) / c2)\n",
    "prior_probs = jnp.array([0.5, 0.5])\n",
    "policy_score = jnp.sqrt(node_visit) * pb_c * prior_probs / (visit_counts + 1)\n",
    "\n",
    "value_score = jnp.array([0, 90])\n",
    "\n",
    "print(policy_score)\n",
    "print(value_score)\n",
    "print(policy_score + value_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 0. 0. 5. 5.]\n",
      "[-inf   1.   2.   3. -inf -inf   5.   5.]\n",
      "[-inf   1.   2.   3. -inf -inf   5.   5.]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "a = jnp.array([0,1,2,3,0,0,5,5]).astype(float)\n",
    "\n",
    "print(a)\n",
    "print(jnp.where(a, a, -jnp.inf))\n",
    "print(jnp.where(a != 0, a, -jnp.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import optax\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class TrainingState:\n",
    "    params: Any\n",
    "    optimizer_state: Any\n",
    "\n",
    "\n",
    "class MLP(flax.linen.Module):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        super.__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        for i in self.hidden_layer_sizes:\n",
    "            x = flax.linen.Dense(i)(x)\n",
    "            x = flax.linen.relu(x)\n",
    "    \n",
    "        return flax.linen.Dense(1)(x)\n",
    "    \n",
    "mlp = MLP((16,16))\n",
    "dummy_observation = jnp.zeros((1,) + data.x.take(1).shape)\n",
    "params = mlp.init(jax.random.PRNGKey(12), dummy_observation)\n",
    "\n",
    "optimizer = optax.adam(1e-4)\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "training_state = TrainingState(params=params, optimizer_state=optimizer_state)\n",
    "\n",
    "def loss(params, data, key, network):\n",
    "    y = network.apply(params, data.x)\n",
    "    loss = jnp.mean((y-data.label)**2)\n",
    "    return loss, loss\n",
    "\n",
    "loss_fn = partial(loss, network=mlp)\n",
    "\n",
    "def update_fn(loss_fn, optimizer):\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    def f(*args, optimizer_state):\n",
    "        values, grads = grad_fn(*args)\n",
    "        params_update, optimizer_state = optimizer.apply(grads, optimizer_state)\n",
    "        params = optax.apply_updates(params, params_update)\n",
    "        return values, params, optimizer_state\n",
    "    \n",
    "    return f\n",
    "\n",
    "gradient_update_fn = update_fn(loss_fn, optimizer)\n",
    "\n",
    "def training_step(carry, data):\n",
    "    optimizer_state, params, key = carry\n",
    "    key, key_loss = jax.random.split(key)\n",
    "    (loss, metrics), params, optimizer_state = gradient_update_fn(params, data, key_loss, optimizer_state=optimizer_state)\n",
    "    return (optimizer_state, params, key), metrics\n",
    "\n",
    "def training_epoch(training_state, data, key, num_minibatches):\n",
    "    key, key_perm, key_grad = jax.random.split(key, 3)\n",
    "\n",
    "    def convert_data(x):\n",
    "        x = jax.random.permutation(key_perm, x) # TODO unnecessary: data already randomly sampled from buffer\n",
    "        x = jnp.reshape(x, (num_minibatches, -1) + x.shape[1:])\n",
    "        return x\n",
    "\n",
    "    shuffled_data = jax.tree_util.tree_map(convert_data, data)\n",
    "    (optimizer_state, params, _), metrics = jax.lax.scan(\n",
    "        training_step, \n",
    "        (training_state.optimizer_state, training_state.params, key_grad),\n",
    "        shuffled_data,\n",
    "        length=num_minibatches)\n",
    "    new_training_state = TrainingState(params=params, optimizer_state=optimizer_state)\n",
    "    metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n",
    "    return new_training_state, metrics, key\n",
    "\n",
    "training_epoch = jax.jit(partial(training_epoch, num_minibatches=32))\n",
    "\n",
    "key = jax.random.PRNGKey(123)\n",
    "for i in range(10):\n",
    "    training_state, metrics, key = training_epoch(training_state, data, key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
